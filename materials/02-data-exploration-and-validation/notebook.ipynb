{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "date: today\n",
    "format:\n",
    "    email:\n",
    "        toc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and validation\n",
    "\n",
    "In this exercise we will cover how to use Polars and Pandera to explore, tidy, and validate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 - Create a virtual environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new virtual environment using uv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following in the terminal:\n",
    "\n",
    "```bash\n",
    "uv venv\n",
    "source .venv/bin/activate\n",
    "which python\n",
    "uv pip sync requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - load data from database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use `polars` to read the data from the database into a Polars dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "from posit.connect import Client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the database credentials\n",
    "if Path(\".env\").exists():\n",
    "    print(\"loading .env\")\n",
    "    load_dotenv(override=True)\n",
    "\n",
    "uri = os.environ[\"DATABASE_URI_PYTHON\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get your username\n",
    "with Client() as client:\n",
    "    username = client.me.username\n",
    "\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vessel verbose data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessel_verbose = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_verbose_raw;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vessel history data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessel_history = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_history_raw;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")\n",
    "\n",
    "vessel_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the terminal locations data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_locations = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_locations_raw;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")\n",
    "\n",
    "terminal_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the terminal weather data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_weather = pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_weather_raw;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")\n",
    "\n",
    "terminal_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin exploring the data. You will want to understand:\n",
    "\n",
    "- What columns exist in the data?\n",
    "- How do the two data sets relate to one another?\n",
    "- What is the type of each column (e.g. string, number, category, date)?\n",
    "- Which columns could be useful for the model?\n",
    "- What steps will I need to perform to clean the data?\n",
    "\n",
    "**Tips**\n",
    "\n",
    "- Use VS Codes built in data viewer to explore the data.\n",
    "- If you are more comfortable with Pandas, you can convert the polars dataframe into a pandas dataframe (e.g. `df.to_pandas()`).\n",
    "- The polars user guide has great docs on how to use polars: https://docs.pola.rs.\n",
    "\n",
    "üö® We are not performing feature engineering at this stage. But it is a good time to start thinking about what features you can create from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dates and times are not formatted correctly. We can fix this when we tidy the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different vessels are in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.select(pl.col(\"VesselID\"), pl.col(\"VesselName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that each VesselID is unique.\n",
    "vessel_verbose.get_column(\"VesselID\").n_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vessel_verbose.get_column(\"VesselID\").n_unique() == vessel_verbose.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all of the numerical columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.select(pl.selectors.numeric()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the date based columns are integers or floats (e.g. `YearBuilt`). During data tidying we could convert them into a proper date type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all of the string columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.select(pl.selectors.string()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It looks like some missing values are represented with an empty string `\"\"` while others have a `null` value. We may want to make this consistent when we tidy the data.\n",
    "- Some string columns are measurements that should be converted into numeric types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data is missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose.null_count()\n",
    "    .transpose(include_header=True)\n",
    "    .rename({\"column\": \"Column Name\", \"column_0\": \"Missing Rows\"})\n",
    "    .with_columns(\n",
    "        ((pl.col(\"Missing Rows\") / vessel_verbose.shape[0]) * 100)\n",
    "        .round(1)\n",
    "        .alias(\"% Missing\")\n",
    "    )\n",
    "    .sort(\"Missing Rows\", descending=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_locations\n",
    "\n",
    "In the interest of time, we will not explore the `terminal_locations` data set. But you should explore it in the same way as the other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_weather\n",
    "\n",
    "In the interest of time, we will not explore the `terminal_weather` data set. But you should explore it in the same way as the other data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Tidy the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a basic understanding of the data, the next step is to tidy the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the string values and keep only the desired columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_locations_clean = terminal_locations.select(\n",
    "    pl.col(\"TerminalName\").str.to_lowercase().str.strip_chars(),\n",
    "    pl.col(\"TerminalAbbrev\").str.to_uppercase().str.strip_chars(),\n",
    "    pl.col(\"Latitude\"),\n",
    "    pl.col(\"Longitude\"),\n",
    ")\n",
    "\n",
    "terminal_locations_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tidy strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_weather_clean = terminal_weather.with_columns(\n",
    "    pl.col(\"timezone\").str.to_lowercase().str.strip_chars(),\n",
    "    pl.col(\"timezone_abbreviation\").str.to_lowercase().str.strip_chars(),\n",
    "    pl.col(\"terminal_name\").str.to_lowercase().str.strip_chars(),\n",
    ")\n",
    "\n",
    "terminal_weather_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tidy datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is all of the data GMT?\n",
    "terminal_weather_clean.get_column(\"timezone\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_weather_clean = terminal_weather_clean.with_columns(\n",
    "    pl.col(\"time\").str.to_datetime(time_zone=\"GMT\")\n",
    ")\n",
    "\n",
    "terminal_weather_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the length measurements into a numeric value. Again we will use a function to capture this complex logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_measurement_string_to_inches(series: pl.Series) -> pl.Series:\n",
    "    \"\"\"\n",
    "    Convert the measurement string into a float.\n",
    "    \"\"\"\n",
    "    feet = series.str.extract(r\"(\\d+)'\").cast(pl.Int64)\n",
    "    inches = series.str.extract(r'(\\d+)\"').cast(pl.Int64).fill_null(0)\n",
    "    total_inches = feet * 12 + inches\n",
    "    return total_inches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_measurement_string_to_inches(pl.Series(['''78' 8\"''']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_measurement_string_to_inches(pl.Series([\"\"\"64'\"\"\", '''100' 11\"''']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = vessel_verbose.with_columns(\n",
    "    pl.col(\"Beam\", \"Length\", \"Draft\")\n",
    "    .map_batches(convert_measurement_string_to_inches)\n",
    "    .name.suffix(\"Inches\"),\n",
    ").select(pl.col(\"*\").exclude([\"Beam\", \"Length\", \"Draft\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the year columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = vessel_verbose_clean.with_columns(\n",
    "    pl.col(\"YearBuilt\").cast(pl.String).str.to_date(\"%Y\"),\n",
    "    pl.col(\"YearRebuilt\").cast(pl.Int64).cast(pl.String).str.to_date(\"%Y\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.select(\"YearBuilt\", \"YearRebuilt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values for strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = vessel_verbose_clean.with_columns(\n",
    "    pl.col(pl.String).replace(\" \", None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all of the string columns so that they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_verbose_clean = vessel_verbose_clean.with_columns(\n",
    "    (\n",
    "        pl.col(\"VesselName\", \"VesselAbbrev\", \"ClassName\", \"CityBuilt\", \"PropulsionInfo\")\n",
    "        .str.to_lowercase()\n",
    "        .str.strip_chars()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_verbose_clean\n",
    "    .select(\"VesselName\", \"VesselAbbrev\", \"ClassName\", \"CityBuilt\", \"PropulsionInfo\")\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datetimes from strings to polars datetime objects. The logic is pretty complex. So we will abstract it into a function that we can apply to all of the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_to_datetime(series: pl.Series) -> pl.Series:\n",
    "    \"\"\"\n",
    "    Convert the datetime format from wadot into a datetime format that polars\n",
    "    can understand.\n",
    "\n",
    "    >>> convert_string_to_datetime(pl.Series(['/Date(1714547700000-0700)/']))\n",
    "    shape: (1,)\n",
    "    Series: '' [datetime[Œºs, UTC]]\n",
    "    [\n",
    "        2024-05-01 07:15:00 UTC\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Extract the unix time stamp. To work with polars we need the time\n",
    "    # the number of seconds since 1970-01-01 00:00 UTC, so divide by\n",
    "    # 1_000.\n",
    "    unix_timestamp = (\n",
    "        (series.str.extract(r\"/Date\\((\\d{13})[-+]\").cast(pl.Int64) / 1_000)\n",
    "        .cast(pl.Int64)\n",
    "        .cast(pl.String)\n",
    "    )\n",
    "    # Extract the timezone.\n",
    "    timezone = series.str.extract(r\"([-+]\\d{4})\")\n",
    "    # Create a new series that has the timestamp and timezone.\n",
    "    clean_timestamp = unix_timestamp + timezone\n",
    "    # Convert into a datetime.\n",
    "    datetime_series = clean_timestamp.str.to_datetime(\"%s%z\")\n",
    "    return datetime_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_string_to_datetime(pl.Series([\"/Date(1714547700000-0700)/\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = vessel_history.with_columns(\n",
    "    (\n",
    "        pl.col(\"ScheduledDepart\", \"ActualDepart\", \"EstArrival\", \"Date\").map_batches(\n",
    "            convert_string_to_datetime\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize all of the string columns so that they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = vessel_history_clean.with_columns(\n",
    "    pl.col(\"Vessel\", \"Departing\", \"Arriving\").str.to_lowercase().str.strip_chars()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.select(\"Vessel\", \"Departing\", \"Arriving\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was identified that many rows have no \"Arriving\" terminal, or \"EstArrival\" date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_history_clean.filter(\n",
    "        pl.col(\"Arriving\").is_null() | pl.col(\"EstArrival\").is_null()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that it means these ferries were cancelled and drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = vessel_history_clean.filter(\n",
    "    pl.col(\"Arriving\").is_not_null(),\n",
    "    pl.col(\"EstArrival\").is_not_null()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct the names of the \"Departing\" and \"Arriving\" terminals so that they match the values in the `terminal_locations` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_name_mapping = {\n",
    "    \"anacortes\": \"anacortes\",\n",
    "    \"bainbridge\": \"bainbridge island\",\n",
    "    \"bremerton\": \"bremerton\",\n",
    "    \"clinton\": \"clinton\",\n",
    "    \"colman\": \"seattle\",\n",
    "    \"edmonds\": \"edmonds\",\n",
    "    \"fauntleroy\": \"fauntleroy\",\n",
    "    \"friday harbor\": \"friday harbor\",\n",
    "    \"keystone\": \"coupeville\",\n",
    "    \"kingston\": \"kingston\",\n",
    "    \"lopez\": \"lopez island\",\n",
    "    \"mukilteo\": \"mukilteo\",\n",
    "    \"orcas\": \"orcas island\",\n",
    "    \"port townsend\": \"port townsend\",\n",
    "    \"pt. defiance\": \"point defiance\",\n",
    "    \"shaw\": \"shaw island\",\n",
    "    \"sidney b. c.\": \"sidney b.c.\",\n",
    "    \"southworth\": \"southworth\",\n",
    "    \"tahlequah\": \"tahlequah\",\n",
    "    \"vashon\": \"vashon island\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_name_mapping_df = pl.DataFrame(\n",
    "    {\n",
    "        \"OldName\": terminal_name_mapping.keys(),\n",
    "        \"CorrectName\": terminal_name_mapping.values(),\n",
    "    }\n",
    ")\n",
    "\n",
    "terminal_name_mapping_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy the vessel history data so that the terminals are named consistently across all data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = (\n",
    "    vessel_history_clean.join(\n",
    "        terminal_name_mapping_df,\n",
    "        left_on=\"Departing\",\n",
    "        right_on=\"OldName\",\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        coalesce=True\n",
    "    )\n",
    "    .rename({\"CorrectName\": \"DepartingCorrected\"})\n",
    "    .join(\n",
    "        terminal_name_mapping_df,\n",
    "        left_on=\"Arriving\",\n",
    "        right_on=\"OldName\",\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        coalesce=True\n",
    "    )\n",
    "    .rename({\"CorrectName\": \"ArrivingCorrected\"})\n",
    "    .drop([\"Departing\", \"Arriving\"])\n",
    "    .rename({\"DepartingCorrected\": \"Departing\", \"ArrivingCorrected\": \"Arriving\"})\n",
    "    .select(\n",
    "        [\n",
    "            \"VesselId\",\n",
    "            \"Vessel\",\n",
    "            \"Departing\",\n",
    "            \"Arriving\",\n",
    "            \"ScheduledDepart\",\n",
    "            \"ActualDepart\",\n",
    "            \"EstArrival\",\n",
    "            \"Date\",\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidy the vessel history data so that the relationship between vessel history and vessel verbose is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was identified that joins based on `VesselId` are not complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_history_clean.join(\n",
    "        vessel_verbose_clean,\n",
    "        left_on=\"VesselId\",\n",
    "        right_on=\"VesselID\",\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        coalesce=True\n",
    "    )\n",
    "    # Filter to show all of the rows where there was no matching Vessel ID in the\n",
    "    # vessel_verbose_clean DataFrame.\n",
    "    .filter(pl.col(\"VesselAbbrev\").is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, joins based on `Vessel` and `VesselName` are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    vessel_history_clean.join(\n",
    "        vessel_verbose_clean,\n",
    "        left_on=\"Vessel\",\n",
    "        right_on=\"VesselName\",\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        coalesce=True\n",
    "    )\n",
    "    # Filter to show all of the rows where there was no matching Vessel ID in the\n",
    "    # vessel_verbose_clean DataFrame. If no rows are returned, then the join was\n",
    "    # successful.\n",
    "    # .filter(pl.col(\"VesselAbbrev\").is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore we should drop the `VesselId` from the data since it is not correct or useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean = vessel_history_clean.drop(\"VesselId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Validate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous activity we tidied the dataset. For some projects, this may be enough. However, for this project we plan to refresh the data on a regular basis. We would like to gain additional comfort that the data we are using is correct. Data validation can help prove that our data tidying was correct, and find any potential issues if the upstream data changes.\n",
    "\n",
    "[Pandera](https://pandera.readthedocs.io/en/stable/) is a Python library for validating Pandas dataframes. There are two steps:\n",
    "\n",
    "1. Define a schema for your data. For example:\n",
    "   - Define the type for each column\n",
    "   - Confirm if null values are allowed\n",
    "   - Define custom checks\n",
    "2. Run your data through the schema validator.\n",
    "\n",
    "You will find these links useful when defining your schema:\n",
    "\n",
    "- Polars data validation guide: https://pandera.readthedocs.io/en/stable/polars.html#usage\n",
    "- Polars data types: https://pandera.readthedocs.io/en/stable/reference/dtypes.html#polars-dtypes\n",
    "- `pa.Field` API: https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.dataframe.model_components.Field.html#pandera.api.dataframe.model_components.Field\n",
    "- List of built in checks you can use with `pa.Field`: https://pandera.readthedocs.io/en/stable/reference/generated/pandera.api.checks.Check.html#pandera.api.checks.Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working on our real data, lets play around with a \"toy\" example. Take a few minutes and play around with the example below:\n",
    "\n",
    "- Can you run the code as is?\n",
    "- Try channging some of the values in the DataFrame so that the schema validation fails.\n",
    "- Try updating the schema so that it passes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandera.polars as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data to validate\n",
    "df = pl.DataFrame({\n",
    "    \"column1\": [1, 11, 0, 10, 9],\n",
    "    \"column2\": [-1.3, -1.4, -2.9, -10.1, -5.2],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ToySchema(pa.DataFrameModel):\n",
    "    column1: int = pa.Field(le=11)\n",
    "    column2: float = pa.Field(lt=1.2)\n",
    "    column3: str = pa.Field(str_startswith=\"value_\")\n",
    "\n",
    "    @pa.check(\"column3\")\n",
    "    def has_one_underscore(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        list_is_len_2 = (\n",
    "            data\n",
    "            .lazyframe\n",
    "            .select(\n",
    "                pl.col(data.key).str.split(\"_\").list.len() == 2\n",
    "            )\n",
    "        )\n",
    "        # print(list_is_len_2.collect())\n",
    "        return list_is_len_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ToySchema.validate(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history\n",
    "\n",
    "Start by validating the `vessel_history` data set. As a reminder, here is what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_history_clean.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class below defines the schema and checks for the `vessel_history` data set.\n",
    "\n",
    "- Each column is a class attribute. At a minimum, we define the column type (e.g. int, str, datetime, etc.)\n",
    "- For some columns, we use `pa.Field` to add more checks. For example in the `EstArrival` column we are going to allow nullable values.\n",
    "- We can define additional and more complex column and dataframe level checks by defining class methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandera.engines.polars_engine import DateTime, Date, Int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselHistorySchema(pa.DataFrameModel):\n",
    "    Vessel: str\n",
    "    Departing: str\n",
    "    Arriving: str\n",
    "    ScheduledDepart: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"})\n",
    "    ActualDepart: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"})\n",
    "    EstArrival: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"UTC\"})\n",
    "    Date: DateTime = pa.Field(\n",
    "        dtype_kwargs={\"time_zone\": \"UTC\"},\n",
    "        ge=pl.datetime(2024, 3, 1, time_zone=\"America/Vancouver\").dt.convert_time_zone(\n",
    "            \"UTC\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    @pa.dataframe_check\n",
    "    def year_of_date_matches_scheduled_depart(cls, df: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that the year of the Date column matches the year of the\n",
    "        ScheduledDepart column.\n",
    "        \"\"\"\n",
    "        return df.lazyframe.select(\n",
    "            pl.col(\"Date\").dt.year().eq(pl.col(\"ScheduledDepart\").dt.year())\n",
    "        )\n",
    "\n",
    "    @pa.dataframe_check(raise_warning=True)\n",
    "    def estimated_arrival_is_after_scheduled_depart(\n",
    "        cls, df: pa.PolarsData\n",
    "    ) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that the EstArrival date time is always after the ScheduledDepart\n",
    "        date time.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "        \"\"\"\n",
    "        return df.lazyframe.select(pl.col(\"EstArrival\").ge(pl.col(\"ScheduledDepart\")))\n",
    "\n",
    "    @pa.check(\"Vessel\")\n",
    "    def vessel_in_vessel_verbose_data_set(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that all of the vessels in the vessel history data set also exist\n",
    "        in the vessel verbose data set.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "\n",
    "        \"\"\"\n",
    "        vessel_names = vessel_verbose_clean.get_column(\"VesselName\").to_list()\n",
    "        return data.lazyframe.select(pl.col(data.key).is_in(vessel_names))\n",
    "\n",
    "    @pa.check(\"Departing\")\n",
    "    def departing_terminal_in_terminal_data(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that all of the vessels in the vessel history data set also exist\n",
    "        in the vessel verbose data set.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "        \"\"\"\n",
    "        terminals = terminal_locations_clean.get_column(\"TerminalName\").to_list()\n",
    "        return data.lazyframe.select(pl.col(data.key).is_in(terminals))\n",
    "\n",
    "    @pa.check(\"Arriving\")\n",
    "    def arriving_terminal_in_terminal_data(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        \"\"\"\n",
    "        Verify that all of the vessels in the vessel history data set also exist\n",
    "        in the vessel verbose data set.\n",
    "\n",
    "        Note this check is expected to fail, therefore raise_warning=True is\n",
    "        used. In the future we should go back and understand why this check\n",
    "        fails.\n",
    "        \"\"\"\n",
    "        terminals = terminal_locations_clean.get_column(\"TerminalName\").to_list()\n",
    "        return data.lazyframe.select(pl.col(data.key).is_in(terminals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate the data, run the dataframe through the `pa.DataFrameModel.validate` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VesselHistorySchema.validate(vessel_history_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are there any more checks that you would add?\n",
    "- How should we handle the data that fails the two checks that raise a warning instead of fail?\n",
    "- Try changing some of the validations so that they fail? Are you able to use the failure message to identify the bad data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose\n",
    "\n",
    "In the interest of time, we will \"skim\" over the validation of the `vessel_verbose` data set. The class below defines the schema and checks for the `vessel_verbose` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*üíÅ Note: time permitting walk the learners through using multiple cursors and split editors in VS Code and how they can be used to quickly create the code for the DataFrame model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the columns into a format we can copy.\n",
    "vessel_verbose_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get each column as a single row, and show the first two examples. This is easy\n",
    "# to read when I am trying to suss out the correct column type and checks.\n",
    "pl.Config.set_tbl_rows(50)\n",
    "vessel_verbose_clean.head(2).transpose(include_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(10)\n",
    "vessel_verbose_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VesselVerboseSchema(pa.DataFrameModel):\n",
    "    VesselID: int\n",
    "    VesselSubjectID: int\n",
    "    VesselName: str = pa.Field(unique=True)\n",
    "    VesselAbbrev: str\n",
    "    ClassID: int\n",
    "    ClassName: str\n",
    "    ClassSubjectID: int\n",
    "    DrawingImg: str\n",
    "    PublicDisplayName: str\n",
    "    SilhouetteImg: str\n",
    "    SortSeq: int\n",
    "    Status: int\n",
    "    OwnedByWSF: bool\n",
    "    CarDeckRestroom: bool\n",
    "    CarDeckShelter: bool\n",
    "    Elevator: bool\n",
    "    ADAAccessible: bool\n",
    "    MainCabinGalley: bool\n",
    "    MainCabinRestroom: bool\n",
    "    PublicWifi: bool\n",
    "    ADAInfo: str\n",
    "    AdditionalInfo: str = pa.Field(nullable=True)\n",
    "    VesselNameDesc: str\n",
    "    VesselHistory: str = pa.Field(nullable=True)\n",
    "    CityBuilt: str\n",
    "    SpeedInKnots: int\n",
    "    EngineCount: int\n",
    "    Horsepower: int\n",
    "    MaxPassengerCount: int\n",
    "    PassengerOnly: bool\n",
    "    FastFerry: bool\n",
    "    PropulsionInfo: str\n",
    "    TallDeckClearance: int\n",
    "    RegDeckSpace: int\n",
    "    TallDeckSpace: int\n",
    "    Tonnage: int\n",
    "    Displacement: int\n",
    "    YearBuilt: Date\n",
    "    YearRebuilt: Date = pa.Field(nullable=True)\n",
    "    SolasCertified: bool\n",
    "    MaxPassengerCountForInternational: int = pa.Field(nullable=True)\n",
    "    BeamInches: int\n",
    "    LengthInches: int\n",
    "    DraftInches: int = pa.Field(nullable=True)\n",
    "\n",
    "    @pa.check(\"DrawingImg\")\n",
    "    def validate_urls(cls, data: pa.PolarsData) -> pl.LazyFrame:\n",
    "        return data.lazyframe.select(pl.col(data.key).str.starts_with(\"https://\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VesselVerboseSchema.validate(vessel_verbose_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, we will \"skim\" over the validation of the this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminalLocationsSchema(pa.DataFrameModel):\n",
    "    TerminalName: str = pa.Field(unique=True)\n",
    "    TerminalAbbrev: str\n",
    "    Latitude: float = pa.Field(ge=-90.0, le=90.0)\n",
    "    Longitude: float = pa.Field(ge=-180.0, le=180.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TerminalLocationsSchema.validate(terminal_locations_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the interest of time, we will \"skim\" over the validation of the this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terminal_weather_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerminalWeatherSchema(pa.DataFrameModel):\n",
    "    latitude: float = pa.Field(ge=-90.0, le=90.0)\n",
    "    longitude: float = pa.Field(ge=-180.0, le=180.0)\n",
    "    generationtime_ms: float\n",
    "    utc_offset_seconds: int\n",
    "    timezone: str = pa.Field(eq=\"gmt\")\n",
    "    timezone_abbreviation: str = pa.Field(eq=\"gmt\")\n",
    "    elevation: float\n",
    "    time: DateTime = pa.Field(dtype_kwargs={\"time_zone\": \"GMT\"}, nullable=True)\n",
    "    weather_code: int\n",
    "    temperature_2m: float\n",
    "    precipitation: float\n",
    "    cloud_cover: int = pa.Field(ge=0, le=100)\n",
    "    wind_speed_10m: float\n",
    "    wind_direction_10m: int = pa.Field(ge=0, le=360)\n",
    "    wind_gusts_10m: float\n",
    "    terminal_name: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TerminalWeatherSchema.validate(terminal_weather_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Write Data to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean data to Posit Connect as a pin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a connection to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database credentials\n",
    "if Path(\".env\").exists():\n",
    "    print(\"loading .env\")\n",
    "    load_dotenv()\n",
    "\n",
    "uri = os.environ[\"DATABASE_URI_PYTHON\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write data to the database\n",
    "vessel_history_clean.write_database(\n",
    "    table_name=f\"{username}_vessel_history_clean\",\n",
    "    connection=uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test that you can read the data\n",
    "pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_history_clean LIMIT 5;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vessel_verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write data to the database\n",
    "vessel_verbose_clean.write_database(\n",
    "    table_name=f\"{username}_vessel_verbose_clean\",\n",
    "    connection=uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test that you can read the data\n",
    "pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_vessel_verbose_clean LIMIT 5;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write data to the database\n",
    "terminal_locations_clean.write_database(\n",
    "    table_name=f\"{username}_terminal_locations_clean\",\n",
    "    connection=uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test that you can read the data\n",
    "pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_locations_clean LIMIT 5;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### terminal_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write data to the database\n",
    "terminal_weather_clean.write_database(\n",
    "    table_name=f\"{username}_terminal_weather_clean\",\n",
    "    connection=uri,\n",
    "    engine=\"adbc\",\n",
    "    if_table_exists='replace'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test that you can read the data\n",
    "pl.read_database_uri(\n",
    "    query=f\"SELECT * FROM {username}_terminal_weather_clean LIMIT 5;\",\n",
    "    uri=uri,\n",
    "    engine=\"adbc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Set up email with Posit Connect and Quarto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posit Connect has support for sending emails with Quarto: https://docs.posit.co/connect/user/quarto/#email-customization.\n",
    "\n",
    "Generate an email to update all the email recipients on the status of the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips**\n",
    "\n",
    "Run the following in the terminal to preview the email:\n",
    "\n",
    "```bash\n",
    "quarto render notebook.ipynb --execute --output-dir tmp\n",
    "```\n",
    "\n",
    "The open `tmp/email-preview/index.html` to preview the email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the variable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "todays_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "todays_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your email template."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "::: {.email}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ":::::: {.subject}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "todays_date",
      "result": {
       "data": {
        "text/plain": "'2024-08-12'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "Seattle Ferry Data Validation Report for `{python} todays_date`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "f\"{username}_terminal_locations_clean\"",
      "result": {
       "data": {
        "text/plain": "'samedwardes_terminal_locations_clean'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{username}_vessel_verbose_clean\"",
      "result": {
       "data": {
        "text/plain": "'samedwardes_vessel_verbose_clean'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{username}_vessel_history_clean\"",
      "result": {
       "data": {
        "text/plain": "'samedwardes_vessel_history_clean'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Update**\n",
    "\n",
    "The Seattle Ferry data has been updated and validated. The following data sets are available for your use:\n",
    "\n",
    "- Terminal Locations: `{python} f\"{username}_terminal_locations_clean\"`\n",
    "- Vessel Verbose: `{python} f\"{username}_vessel_verbose_clean\"`\n",
    "- Vessel History: `{python} f\"{username}_vessel_history_clean\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "f\"{terminal_locations_clean.shape[0]:,}\"",
      "result": {
       "data": {
        "text/plain": "'20'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{terminal_locations_clean.shape[1]:,}\"",
      "result": {
       "data": {
        "text/plain": "'4'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "\", \".join(terminal_locations_clean['TerminalName'].unique())",
      "result": {
       "data": {
        "text/plain": "'point defiance, coupeville, lopez island, port townsend, southworth, vashon island, sidney b.c., tahlequah, bainbridge island, fauntleroy, edmonds, mukilteo, orcas island, bremerton, anacortes, kingston, seattle, shaw island, clinton, friday harbor'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "**Terminal Locations**\n",
    "\n",
    "- `{python} f\"{terminal_locations_clean.shape[0]:,}\"` rows\n",
    "- `{python} f\"{terminal_locations_clean.shape[1]:,}\"` columns\n",
    "- Terminals: `{python} \", \".join(terminal_locations_clean['TerminalName'].unique())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "f\"{vessel_verbose_clean.shape[0]:,}\"",
      "result": {
       "data": {
        "text/plain": "'21'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{vessel_verbose_clean.shape[1]:,}\"",
      "result": {
       "data": {
        "text/plain": "'44'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "\", \".join(vessel_verbose_clean['VesselName'].unique())",
      "result": {
       "data": {
        "text/plain": "'puyallup, issaquah, kennewick, chelan, kitsap, samish, suquamish, walla walla, sealth, chetzemoka, salish, wenatchee, kittitas, yakima, chimacum, kaleetan, spokane, cathlamet, tacoma, tillikum, tokitae'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "**Vessel Verbose**\n",
    "\n",
    "- `{python} f\"{vessel_verbose_clean.shape[0]:,}\"` rows\n",
    "- `{python} f\"{vessel_verbose_clean.shape[1]:,}\"` columns\n",
    "- Vessels: `{python} \", \".join(vessel_verbose_clean['VesselName'].unique())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": [
     {
      "expression": "f\"{vessel_history_clean.shape[0]:,}\"",
      "result": {
       "data": {
        "text/plain": "'55,794'"
       },
       "metadata": {},
       "status": "ok"
      }
     },
     {
      "expression": "f\"{vessel_history_clean.shape[1]:,}\"",
      "result": {
       "data": {
        "text/plain": "'7'"
       },
       "metadata": {},
       "status": "ok"
      }
     }
    ]
   },
   "source": [
    "**Vessel History**\n",
    "\n",
    "- `{python} f\"{vessel_history_clean.shape[0]:,}\"` rows\n",
    "- `{python} f\"{vessel_history_clean.shape[1]:,}\"` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | echo: false\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "ax = (\n",
    "    vessel_history_clean.with_columns(\n",
    "        pl.col(\"Date\").dt.date().dt.month_start().alias(\"Month\"),\n",
    "    )\n",
    "    .group_by(\"Month\")\n",
    "    .agg(pl.col(\"Vessel\").count().alias(\"Trips\"))\n",
    "    .sort(\"Month\")\n",
    "    .to_pandas()\n",
    "    .plot(\n",
    "        x=\"Month\",\n",
    "        y=\"Trips\",\n",
    "        title=\"Trips by Month\",\n",
    "    )\n",
    ")\n",
    "\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:,.0f}\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 - publish notebook as Quarto document to Posit Connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÑ Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy the notebook to Posit Connect as a Quarto document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßë‚Äçüíª Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posit Publisher**\n",
    "\n",
    "Deploy using the Posit Publisher VS Code extension. Check that you have the required environment variables:\n",
    "\n",
    "```bash\n",
    "source .env\n",
    "echo $CONNECT_SERVER\n",
    "echo $CONNECT_API_KEY\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rsconnect CLI**\n",
    "\n",
    "Run the following to deploy the notebook to Connect:\n",
    "\n",
    "```bash\n",
    "# Check that you have the required environment variables set\n",
    "source .env\n",
    "echo $CONNECT_SERVER\n",
    "echo $CONNECT_API_KEY\n",
    "echo $DATABASE_URI_PYTHON\n",
    "\n",
    "# Publish the notebook\n",
    "rsconnect deploy quarto --title \"Seattle Ferries #2 - Data exploration and validation\" -E DATABASE_URI_PYTHON notebook.ipynb\n",
    "```\n",
    "\n",
    "After the deployment is successful:\n",
    "\n",
    "- Share the notebook with the person beside you.\n",
    "- Schedule the notebook to run once every week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Notebook complete ‚úÖ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/home/samedwardes/.local/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
